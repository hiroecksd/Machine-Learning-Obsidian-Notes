# Markov Process/Chain
A **Markov process** is a memoryless random process. It is a process for which predictions can be made regarding future outcomes based solely on its present state and most importantly such prediction are just as good as the ones that could be made knowing processes full history. Conditional on the present state of the system, and past states are independent. Because we have not formally defined what we mean by dynamics/transitions I will do it here. A **transition** are the changes of state of the system. We define $S$ as a finite set of states with $s \in S$. Then we can define $P$ as the transition or dynamics model which specifies $$ p(s_{t + 1} = s' | s_{t} = s)$$ One thing you should know that if there is no rewards, there will be no actions. Given that we have a finite number ($N$) of states, the transition model $P$ can be expressed as a matrix: $$ P = \begin{pmatrix} P(s_{1} | s_{1}) & \cdots & P(s_{N} | s_{1}) \\ \vdots & \ddots & \vdots \\ P(S_{1} | s_{N}) & \cdots & P(s_{N} | s_{N}) \end{pmatrix} $$
# Markov Reward Process (MRP)
An MRP is a Markov chain that has rewards. Including the variables we denoted earlier we have two more to add. The Reward function ($R$) which is given by: $$R(s{t} = s) = \mathbb{E}[r_{t}|s_{t} = s]$$ and mentioned in previous notes you saw the discount factor $\gamma \in [0,1]$. If there is a finite number ($N$) of states, we can express $R$ as a vector. There are no actions here. 
## Return and Value Function
